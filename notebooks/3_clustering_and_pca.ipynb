{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Analysis with Data Science & Machine Learning - Part 3\n",
    "## Clustering Analysis and Principal Component Analysis (PCA)\n",
    "\n",
    "This notebook applies unsupervised learning techniques to identify patterns and group companies based on their financial characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the data with financial ratios from the previous notebook\n",
    "try:\n",
    "    data = pd.read_csv('financial_data_with_ratios.csv')\n",
    "    print(f\"Successfully loaded data with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Financial data with ratios file not found. Please run the previous notebooks first.\")\n",
    "    # Try to load the cleaned data if the ratios file is not available\n",
    "    try:\n",
    "        data = pd.read_csv('cleaned_financial_data.csv')\n",
    "        print(f\"Loaded cleaned data instead with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No data files found. Please run the previous notebooks to generate the necessary data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to prepare data for clustering and PCA\n",
    "def prepare_data_for_ml(df):\n",
    "    \"\"\"Prepare financial data for machine learning analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Dataframe containing financial data and ratios\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_scaled, feature_names, df_prepared)\n",
    "        - X_scaled: Scaled features for ML\n",
    "        - feature_names: List of feature names\n",
    "        - df_prepared: Processed dataframe with company identifiers\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    df_ml = df.copy()\n",
    "    \n",
    "    # Identify categorical and identifier columns\n",
    "    id_cols = ['Company Name', 'Ticker']\n",
    "    cat_cols = ['Sector'] if 'Sector' in df_ml.columns else []\n",
    "    \n",
    "    # List of columns that should be available in the dataset\n",
    "    # Add additional financial ratios if they exist\n",
    "    potential_feature_cols = [\n",
    "        'Total Revenue', 'Gross Profit', 'Operating Income', 'Net Income',\n",
    "        'Total Assets', 'Total Liabilities', 'Equity', 'Cash and Cash Equivalents',\n",
    "        'Market Capitalization', 'ROA', 'ROE', 'Net_Margin', 'Gross_Margin',\n",
    "        'Operating_Margin', 'Debt_to_Equity', 'Debt_Ratio', 'Asset_Turnover'\n",
    "    ]\n",
    "    \n",
    "    # Select only features that exist in the dataframe\n",
    "    feature_cols = [col for col in potential_feature_cols if col in df_ml.columns]\n",
    "    \n",
    "    if not feature_cols:\n",
    "        raise ValueError(\"No valid feature columns found in the dataset\")\n",
    "    \n",
    "    print(f\"Selected {len(feature_cols)} features for analysis\")\n",
    "    \n",
    "    # Check for missing values in the selected features\n",
    "    missing_values = df_ml[feature_cols].isnull().sum()\n",
    "    features_with_missing = missing_values[missing_values > 0]\n",
    "    \n",
    "    if not features_with_missing.empty:\n",
    "        print(\"Features with missing values:\")\n",
    "        print(features_with_missing)\n",
    "        \n",
    "        # Impute missing values using median\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        df_ml[feature_cols] = imputer.fit_transform(df_ml[feature_cols])\n",
    "        print(\"Missing values imputed with median\")\n",
    "    \n",
    "    # Select relevant columns for ML\n",
    "    # Convert to numeric and replace infinite values with NaN\n",
    "    X = df_ml[feature_cols].apply(pd.to_numeric, errors='coerce').replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Drop any rows with NaN values after conversion\n",
    "    rows_before = X.shape[0]\n",
    "    X = X.dropna()\n",
    "    rows_after = X.shape[0]\n",
    "    \n",
    "    if rows_before > rows_after:\n",
    "        print(f\"Dropped {rows_before - rows_after} rows with NaN values\")\n",
    "        # Get indices of valid rows\n",
    "        valid_indices = X.index\n",
    "        # Filter the original dataframe\n",
    "        df_ml = df_ml.loc[valid_indices]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"Prepared data shape: {X_scaled.shape}\")\n",
    "    \n",
    "    # Create a dataframe with company identifiers and scaled features\n",
    "    id_df = df_ml[id_cols + cat_cols] if cat_cols else df_ml[id_cols]\n",
    "    \n",
    "    return X_scaled, feature_cols, id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for machine learning\n",
    "try:\n",
    "    X_scaled, feature_names, companies = prepare_data_for_ml(data)\n",
    "    print(f\"Data prepared successfully with {len(feature_names)} features for {X_scaled.shape[0]} companies\")\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform PCA to reduce dimensionality\n",
    "def perform_pca(X_scaled, feature_names, n_components=0.95):\n",
    "    \"\"\"Perform PCA on scaled financial data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_scaled : numpy array\n",
    "        Scaled features for PCA\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    n_components : int or float, optional (default=0.95)\n",
    "        Number of components to keep or variance threshold\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (pca, X_pca)\n",
    "        - pca: Fitted PCA model\n",
    "        - X_pca: Transformed data\n",
    "    \"\"\"\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Print explained variance\n",
    "    print(f\"PCA with {pca.n_components_} components explains {pca.explained_variance_ratio_.sum()*100:.2f}% of the variance\")\n",
    "    \n",
    "    return pca, X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform PCA\n",
    "try:\n",
    "    pca, X_pca = perform_pca(X_scaled, feature_names)\n",
    "    \n",
    "    # Display the explained variance ratio\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance by Principal Component')\n",
    "    plt.xticks(range(1, pca.n_components_ + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Cumulative explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, pca.n_components_ + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Cumulative Explained Variance by Component')\n",
    "    plt.axhline(y=0.95, color='r', linestyle='-', label='95% Explained Variance')\n",
    "    plt.xticks(range(1, pca.n_components_ + 1))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error performing PCA: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze PCA components\n",
    "try:\n",
    "    # Create a dataframe of component loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "        index=feature_names\n",
    "    )\n",
    "    \n",
    "    # Display loadings for the first few principal components\n",
    "    print(\"PCA Component Loadings (first 3 components):\")\n",
    "    loadings.iloc[:, :3]\n",
    "    \n",
    "    # Visualize component loadings for top 2 components\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(loadings.iloc[:, :2], annot=True, cmap='coolwarm', fmt='.3f')\n",
    "    plt.title('Feature Loadings for First Two Principal Components')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify important features for each component\n",
    "    for i in range(min(3, pca.n_components_)):\n",
    "        component = loadings[f'PC{i+1}']\n",
    "        # Sort by absolute loading value\n",
    "        sorted_loadings = component.abs().sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"\\nPC{i+1} - Top 5 features with highest absolute loadings:\")\n",
    "        for feature, loading in sorted_loadings.head(5).items():\n",
    "            actual_loading = component[feature]\n",
    "            print(f\"{feature}: {actual_loading:.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing PCA components: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "try:\n",
    "    # Create figure for the elbow method visualizer\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Instantiate the KElbowVisualizer with KMeans\n",
    "    visualizer = KElbowVisualizer(KMeans(random_state=42), k=(2, 10))\n",
    "    \n",
    "    # Fit the visualizer\n",
    "    visualizer.fit(X_scaled)\n",
    "    visualizer.show()\n",
    "    \n",
    "    # Get the optimal number of clusters\n",
    "    optimal_k = visualizer.elbow_value_\n",
    "    print(f\"Optimal number of clusters (K): {optimal_k}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error determining optimal clusters: {e}\")\n",
    "    # Set a default value if the elbow method fails\n",
    "    optimal_k = 3\n",
    "    print(f\"Using default number of clusters (K): {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform K-Means clustering with the optimal number of clusters\n",
    "try:\n",
    "    # Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add cluster labels to the companies dataframe\n",
    "    companies_with_clusters = companies.copy()\n",
    "    companies_with_clusters['Cluster'] = clusters\n",
    "    \n",
    "    # Count companies in each cluster\n",
    "    cluster_counts = companies_with_clusters['Cluster'].value_counts().sort_index()\n",
    "    print(\"\\nCompanies per cluster:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"Cluster {cluster}: {count} companies\")\n",
    "    \n",
    "    # Visualize the clusters in 2D PCA space\n",
    "    if X_pca.shape[1] >= 2:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.title('K-Means Clusters in PCA Space')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        \n",
    "        # Add cluster centers (transformed to PCA space)\n",
    "        centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "        plt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=200, marker='X', c='red', label='Centroids')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # If we have 3 PCA components, create a 3D visualization\n",
    "        if X_pca.shape[1] >= 3:\n",
    "            fig = px.scatter_3d(\n",
    "                x=X_pca[:, 0], y=X_pca[:, 1], z=X_pca[:, 2],\n",
    "                color=clusters.astype(str),\n",
    "                title='K-Means Clusters in 3D PCA Space',\n",
    "                labels={'x': 'PC1', 'y': 'PC2', 'z': 'PC3'}\n",
    "            )\n",
    "            fig.update_traces(marker=dict(size=5))\n",
    "            fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error performing clustering: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster Analysis and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze clusters by looking at the average values of features for each cluster\n",
    "try:\n",
    "    # Combine original data with cluster labels\n",
    "    data_with_clusters = data.loc[companies.index].copy()\n",
    "    data_with_clusters['Cluster'] = clusters\n",
    "    \n",
    "    # Calculate average feature values for each cluster\n",
    "    cluster_profiles = data_with_clusters.groupby('Cluster')[feature_names].mean()\n",
    "    \n",
    "    # Display cluster profiles\n",
    "    print(\"Cluster profiles (average values of features):\")\n",
    "    cluster_profiles\n",
    "    \n",
    "    # Visualize key features across clusters\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    # Standardize the values for better visualization\n",
    "    cluster_profiles_scaled = (cluster_profiles - cluster_profiles.mean()) / cluster_profiles.std()\n",
    "    sns.heatmap(cluster_profiles_scaled, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Standardized Feature Values by Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Radar chart for clusters (using a subset of features for clarity)\n",
    "    # Select top features based on variance between clusters\n",
    "    features_variance = cluster_profiles.var()\n",
    "    top_features = features_variance.sort_values(ascending=False).head(6).index.tolist()\n",
    "    \n",
    "    # Create radar chart data\n",
    "    categories = top_features\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for cluster in cluster_profiles.index:\n",
    "        values = cluster_profiles.loc[cluster, top_features].values.tolist()\n",
    "        # Close the polygon by repeating the first value\n",
    "        values.append(values[0])\n",
    "        categories_closed = categories + [categories[0]]\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=categories_closed,\n",
    "            name=f'Cluster {cluster}'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True)),\n",
    "        title='Key Features by Cluster - Radar Chart',\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing clusters: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze sectors by cluster (if sector information is available)\n",
    "if 'Sector' in companies.columns:\n",
    "    try:\n",
    "        # Create a dataframe with sector and cluster information\n",
    "        sector_cluster = companies.copy()\n",
    "        sector_cluster['Cluster'] = clusters\n",
    "        \n",
    "        # Cross-tabulation of sectors and clusters\n",
    "        sector_cluster_counts = pd.crosstab(sector_cluster['Sector'], sector_cluster['Cluster'])\n",
    "        \n",
    "        # Calculate percentages within each cluster\n",
    "        sector_cluster_pct = sector_cluster_counts.div(sector_cluster_counts.sum(axis=0), axis=1) * 100\n",
    "        \n",
    "        print(\"Sector distribution by cluster (percentages):\")\n",
    "        sector_cluster_pct\n",
    "        \n",
    "        # Visualize sector distribution by cluster\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        sns.heatmap(sector_cluster_pct, annot=True, cmap='YlGnBu', fmt='.1f')\n",
    "        plt.title('Sector Distribution by Cluster (%)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sectors by cluster: {e}\")\n",
    "else:\n",
    "    print(\"No sector information available in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Clustered Data for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the data with cluster assignments\n",
    "try:\n",
    "    # Create a dataframe with all data and cluster assignments\n",
    "    full_data_with_clusters = data.loc[companies.index].copy()\n",
    "    full_data_with_clusters['Cluster'] = clusters\n",
    "    \n",
    "    # Add PCA components if available\n",
    "    for i in range(min(3, X_pca.shape[1])):\n",
    "        full_data_with_clusters[f'PC{i+1}'] = X_pca[:, i]\n",
    "    \n",
    "    # Save to CSV\n",
    "    full_data_with_clusters.to_csv('financial_data_with_clusters.csv', index=False)\n",
    "    print(\"Saved data with cluster assignments to 'financial_data_with_clusters.csv'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving clustered data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "In this notebook, we have:\n",
    "1. Prepared the financial data for machine learning analysis\n",
    "2. Applied Principal Component Analysis (PCA) to identify the key dimensions of financial variation\n",
    "3. Used K-Means clustering to segment companies into groups with similar financial characteristics\n",
    "4. Analyzed and interpreted the resulting clusters\n",
    "\n",
    "Key insights:\n",
    "- [The notebook will generate insights based on the actual data]\n",
    "- [For example: Cluster 0 might represent high-growth companies with high ROE and low debt]\n",
    "- [Cluster 1 might represent stable companies with moderate growth and low leverage]\n",
    "- [Cluster 2 might represent companies with higher debt ratios but good profit margins]\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we will:\n",
    "1. Apply supervised learning techniques (Decision Trees, Random Forests, Regression models)\n",
    "2. Predict important financial variables\n",
    "3. Identify the key factors that determine financial performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

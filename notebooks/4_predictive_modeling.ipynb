{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Analysis with Data Science & Machine Learning - Part 4\n",
    "## Predictive Modeling with Supervised Learning\n",
    "\n",
    "This notebook applies supervised learning techniques to identify key factors that determine financial performance and predict financial variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import graphviz\n",
    "from sklearn import tree\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "# Display all columns and rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the data with cluster assignments from the previous notebook\n",
    "try:\n",
    "    data = pd.read_csv('financial_data_with_clusters.csv')\n",
    "    print(f\"Successfully loaded data with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    # Try to load the data with ratios if the clustered data is not available\n",
    "    try:\n",
    "        data = pd.read_csv('financial_data_with_ratios.csv')\n",
    "        print(f\"Loaded data with ratios instead with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "    except FileNotFoundError:\n",
    "        # Try to load the cleaned data as a last resort\n",
    "        try:\n",
    "            data = pd.read_csv('cleaned_financial_data.csv')\n",
    "            print(f\"Loaded cleaned data instead with {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No data files found. Please run the previous notebooks to generate the necessary data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection and Target Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define target variables and features for prediction\n",
    "def prepare_features_targets(df, target_col):\n",
    "    \"\"\"Prepare features and target for supervised learning\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Dataset containing financial data\n",
    "    target_col : str\n",
    "        Name of the target column to predict\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_train, X_test, y_train, y_test, feature_names)\n",
    "        - Training and testing data splits\n",
    "        - List of feature names\n",
    "    \"\"\"\n",
    "    # Copy the dataframe\n",
    "    df_ml = df.copy()\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if target_col not in df_ml.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in the dataset\")\n",
    "    \n",
    "    # Define columns to exclude from features\n",
    "    exclude_cols = [\n",
    "        'Company Name', 'Ticker', 'Sector',  # Identifiers\n",
    "        target_col,                          # Target\n",
    "        'Cluster'                            # Cluster assignment (if exists)\n",
    "    ]\n",
    "    \n",
    "    # Also exclude PCA components if they exist\n",
    "    pca_cols = [col for col in df_ml.columns if col.startswith('PC')]\n",
    "    exclude_cols.extend(pca_cols)\n",
    "    \n",
    "    # Filter out columns that don't exist\n",
    "    exclude_cols = [col for col in exclude_cols if col in df_ml.columns]\n",
    "    \n",
    "    # Select feature columns (all numeric columns except excluded ones)\n",
    "    numeric_cols = df_ml.select_dtypes(include=np.number).columns\n",
    "    feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    if not feature_cols:\n",
    "        raise ValueError(\"No valid feature columns found after filtering\")\n",
    "    \n",
    "    print(f\"Selected {len(feature_cols)} features for predicting {target_col}\")\n",
    "    \n",
    "    # Handle missing values in features and target\n",
    "    # Drop rows with missing values in target column\n",
    "    df_ml = df_ml.dropna(subset=[target_col])\n",
    "    \n",
    "    # For features, fill missing values with median\n",
    "    for col in feature_cols:\n",
    "        if df_ml[col].isnull().sum() > 0:\n",
    "            median_val = df_ml[col].median()\n",
    "            df_ml[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df_ml[feature_cols]\n",
    "    y = df_ml[target_col]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test_scaled.shape[0]} samples\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the target variable to predict (select one financial metric)\n",
    "# Examples: 'Net Income', 'ROE', 'Market Capitalization'\n",
    "target_variable = 'Net Income'  # Change this as needed\n",
    "\n",
    "try:\n",
    "    # Check if the target exists in the dataset\n",
    "    if target_variable not in data.columns:\n",
    "        print(f\"Target variable '{target_variable}' not found in the dataset\")\n",
    "        # List available numerical columns as potential targets\n",
    "        num_cols = data.select_dtypes(include=np.number).columns.tolist()\n",
    "        print(\"Available numerical columns for prediction:\")\n",
    "        print(num_cols)\n",
    "        # Select first available suitable target if original target not found\n",
    "        potential_targets = ['Net Income', 'ROE', 'Operating Income', 'Gross Profit', 'Total Revenue']\n",
    "        for potential in potential_targets:\n",
    "            if potential in data.columns:\n",
    "                target_variable = potential\n",
    "                print(f\"Using '{target_variable}' as the target variable instead\")\n",
    "                break\n",
    "    \n",
    "    # Prepare data for modeling\n",
    "    X_train, X_test, y_train, y_test, feature_names = prepare_features_targets(\n",
    "        data, target_variable\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing data for modeling: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train a Decision Tree model\n",
    "try:\n",
    "    # Initialize and train a Decision Tree model\n",
    "    dt_model = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred_dt = dt_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "    rmse_dt = np.sqrt(mse_dt)\n",
    "    r2_dt = r2_score(y_test, y_pred_dt)\n",
    "    \n",
    "    print(f\"Decision Tree Performance for predicting {target_variable}:\")\n",
    "    print(f\"Root Mean Squared Error: {rmse_dt:.2f}\")\n",
    "    print(f\"R² Score: {r2_dt:.4f}\")\n",
    "    \n",
    "    # Visualize the Decision Tree\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(dt_model, feature_names=feature_names, filled=True, rounded=True, fontsize=10)\n",
    "    plt.title(f\"Decision Tree for {target_variable} Prediction\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a more detailed visualization with GraphViz\n",
    "    dot_data = tree.export_graphviz(\n",
    "        dt_model,\n",
    "        out_file=None,\n",
    "        feature_names=feature_names,\n",
    "        filled=True,\n",
    "        rounded=True\n",
    "    )\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph\n",
    "except Exception as e:\n",
    "    print(f\"Error training Decision Tree model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train a Random Forest model\n",
    "try:\n",
    "    # Initialize and train a Random Forest model\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "    rmse_rf = np.sqrt(mse_rf)\n",
    "    r2_rf = r2_score(y_test, y_pred_rf)\n",
    "    \n",
    "    print(f\"Random Forest Performance for predicting {target_variable}:\")\n",
    "    print(f\"Root Mean Squared Error: {rmse_rf:.2f}\")\n",
    "    print(f\"R² Score: {r2_rf:.4f}\")\n",
    "    \n",
    "    # Extract feature importance\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Display feature importance\n",
    "    print(\"\\nFeature importance:\")\n",
    "    feature_importance_df.head(10)  # Show top 10 features\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10))\n",
    "    plt.title(f\"Top 10 Features for Predicting {target_variable}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error training Random Forest model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train Ridge and Lasso regression models\n",
    "try:\n",
    "    # Initialize and train Ridge Regression model with cross-validation\n",
    "    ridge_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "    ridge_cv = GridSearchCV(Ridge(random_state=42), ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
    "    ridge_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Best Ridge model\n",
    "    best_ridge = ridge_cv.best_estimator_\n",
    "    ridge_alpha = ridge_cv.best_params_['alpha']\n",
    "    y_pred_ridge = best_ridge.predict(X_test)\n",
    "    \n",
    "    # Initialize and train Lasso Regression model with cross-validation\n",
    "    lasso_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "    lasso_cv = GridSearchCV(Lasso(random_state=42), lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
    "    lasso_cv.fit(X_train, y_train)\n",
    "    \n",
    "    # Best Lasso model\n",
    "    best_lasso = lasso_cv.best_estimator_\n",
    "    lasso_alpha = lasso_cv.best_params_['alpha']\n",
    "    y_pred_lasso = best_lasso.predict(X_test)\n",
    "    \n",
    "    # Evaluate the models\n",
    "    print(\"\\nRidge Regression:\")\n",
    "    print(f\"Best alpha: {ridge_alpha}\")\n",
    "    ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "    ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
    "    print(f\"RMSE: {ridge_rmse:.2f}\")\n",
    "    print(f\"R²: {ridge_r2:.4f}\")\n",
    "    \n",
    "    print(\"\\nLasso Regression:\")\n",
    "    print(f\"Best alpha: {lasso_alpha}\")\n",
    "    lasso_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "    lasso_r2 = r2_score(y_test, y_pred_lasso)\n",
    "    print(f\"RMSE: {lasso_rmse:.2f}\")\n",
    "    print(f\"R²: {lasso_r2:.4f}\")\n",
    "    \n",
    "    # Compare coefficients\n",
    "    ridge_coef = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Ridge_Coefficient': best_ridge.coef_\n",
    "    })\n",
    "    \n",
    "    lasso_coef = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Lasso_Coefficient': best_lasso.coef_\n",
    "    })\n",
    "    \n",
    "    # Merge coefficients\n",
    "    coef_df = pd.merge(ridge_coef, lasso_coef, on='Feature')\n",
    "    \n",
    "    # Sort by absolute Ridge coefficient\n",
    "    coef_df['Abs_Ridge'] = np.abs(coef_df['Ridge_Coefficient'])\n",
    "    coef_df = coef_df.sort_values('Abs_Ridge', ascending=False).drop('Abs_Ridge', axis=1)\n",
    "    \n",
    "    # Display top coefficients\n",
    "    print(\"\\nTop coefficients:\")\n",
    "    coef_df.head(10)\n",
    "    \n",
    "    # Visualize coefficients\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_coef = coef_df.head(10).copy()\n",
    "    \n",
    "    # Melt the dataframe for easier plotting\n",
    "    top_coef_melted = pd.melt(\n",
    "        top_coef, \n",
    "        id_vars=['Feature'], \n",
    "        value_vars=['Ridge_Coefficient', 'Lasso_Coefficient'],\n",
    "        var_name='Model', \n",
    "        value_name='Coefficient'\n",
    "    )\n",
    "    \n",
    "    # Create the plot\n",
    "    g = sns.catplot(\n",
    "        data=top_coef_melted, \n",
    "        kind='bar',\n",
    "        x='Coefficient', \n",
    "        y='Feature', \n",
    "        hue='Model',\n",
    "        height=8, \n",
    "        aspect=1.5\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Top 10 Coefficients for Predicting {target_variable}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error training regression models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare model performance\n",
    "try:\n",
    "    # Create a dataframe of model performance metrics\n",
    "    model_comparison = pd.DataFrame({\n",
    "        'Model': ['Decision Tree', 'Random Forest', 'Ridge Regression', 'Lasso Regression'],\n",
    "        'RMSE': [rmse_dt, rmse_rf, ridge_rmse, lasso_rmse],\n",
    "        'R²': [r2_dt, r2_rf, ridge_r2, lasso_r2]\n",
    "    })\n",
    "    \n",
    "    # Sort by R² (higher is better)\n",
    "    model_comparison = model_comparison.sort_values('R²', ascending=False)\n",
    "    \n",
    "    print(\"Model performance comparison:\")\n",
    "    model_comparison\n",
    "    \n",
    "    # Visualize model comparison\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='R²', y='Model', data=model_comparison)\n",
    "    plt.title(f\"Model Performance Comparison for {target_variable} Prediction (R²)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # RMSE comparison\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.barplot(x='RMSE', y='Model', data=model_comparison.sort_values('RMSE'))\n",
    "    plt.title(f\"Model Performance Comparison for {target_variable} Prediction (RMSE)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error comparing models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Financial Drivers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract and analyze key financial drivers from the models\n",
    "try:\n",
    "    # Combine importance/coefficients from all models\n",
    "    # Random Forest feature importance\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'RF_Importance': rf_model.feature_importances_\n",
    "    })\n",
    "    \n",
    "    # Ridge and Lasso coefficients (absolute values for fair comparison)\n",
    "    ridge_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Ridge_Importance': np.abs(best_ridge.coef_)\n",
    "    })\n",
    "    \n",
    "    lasso_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Lasso_Importance': np.abs(best_lasso.coef_)\n",
    "    })\n",
    "    \n",
    "    # Merge all metrics\n",
    "    drivers_df = rf_importance.merge(ridge_importance, on='Feature')\n",
    "    drivers_df = drivers_df.merge(lasso_importance, on='Feature')\n",
    "    \n",
    "    # Normalize each importance/coefficient column to [0,1] for fair comparison\n",
    "    for col in ['RF_Importance', 'Ridge_Importance', 'Lasso_Importance']:\n",
    "        if drivers_df[col].sum() > 0:  # Avoid division by zero\n",
    "            drivers_df[col] = drivers_df[col] / drivers_df[col].sum()\n",
    "    \n",
    "    # Calculate average importance across models\n",
    "    drivers_df['Average_Importance'] = drivers_df[['RF_Importance', 'Ridge_Importance', 'Lasso_Importance']].mean(axis=1)\n",
    "    \n",
    "    # Sort by average importance\n",
    "    drivers_df = drivers_df.sort_values('Average_Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop financial drivers across models:\")\n",
    "    drivers_df.head(10)\n",
    "    \n",
    "    # Visualize top drivers\n",
    "    top_drivers = drivers_df.head(10)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Melt the dataframe for easier plotting\n",
    "    top_drivers_melted = pd.melt(\n",
    "        top_drivers, \n",
    "        id_vars=['Feature'], \n",
    "        value_vars=['RF_Importance', 'Ridge_Importance', 'Lasso_Importance'],\n",
    "        var_name='Model', \n",
    "        value_name='Importance'\n",
    "    )\n",
    "    \n",
    "    # Create the plot\n",
    "    sns.barplot(x='Importance', y='Feature', hue='Model', data=top_drivers_melted)\n",
    "    plt.title(f\"Top 10 Financial Drivers for {target_variable}\")\n",
    "    plt.legend(title='Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing financial drivers: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "In this notebook, we have:\n",
    "1. Prepared financial data for supervised learning\n",
    "2. Built and evaluated predictive models for financial performance\n",
    "3. Identified key financial drivers using various machine learning techniques\n",
    "4. Compared the performance of different modeling approaches\n",
    "\n",
    "Key insights:\n",
    "- [The notebook will generate insights based on the actual data]\n",
    "- [For example: The most important factors for predicting Net Income might be ...]\n",
    "- [Model performance suggests that Random Forest provides the best balance of accuracy and interpretability]\n",
    "- [The identified financial drivers align with established financial theory by showing ...]\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we will:\n",
    "1. Combine insights from clustering and predictive modeling\n",
    "2. Develop comprehensive economic interpretations of our findings\n",
    "3. Generate actionable recommendations based on the analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
